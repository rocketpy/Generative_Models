# DeepSeek-V3 a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. 

# https://github.com/deepseek-ai/DeepSeek-V3
